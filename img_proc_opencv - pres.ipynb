{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing using Python\n",
    "\n",
    "Python has many Image processing Libraries:\n",
    "\n",
    "[scipy.ndimage](https://docs.scipy.org/doc/scipy/tutorial/ndimage.html)\n",
    "\n",
    "[scikit-image](https://scikit-image.org/)\n",
    "\n",
    "[Pillow](https://python-pillow.org/)\n",
    "\n",
    "[OpenCV](https://opencv.org/)\n",
    "\n",
    "OpenCV is a free and open-source computer vision library. OpenCV is written in optimized\n",
    "C++, but it provides Python wrappers. Therefore, this library can be used in your Python\n",
    "programs. `opencv-python` is the Python package that contains pre-built OpenCV with dependencies and Python bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV-Python Installation\n",
    "From the Anaconda prompt, give the command: `pip install opencv-python`\n",
    "\n",
    "More details on installation [here](https://github.com/opencv/opencv-python)\n",
    "\n",
    "Link to OpenCV-Python Tutorial [here](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import opencv\n",
    "# OpenCV's Python module is called cv2 even though we are using\n",
    "# OpenCV 4.x and not OpenCV 2.x. Historically, OpenCV had two Python\n",
    "# modules: cv2 and cv. The latter wrapped a legacy version of OpenCV\n",
    "# implemented in C. Nowadays, OpenCV has only the cv2 Python module,\n",
    "# which wraps the current version of OpenCV implemented in C++.\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an image using `cv2.imread()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/logo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dimensions of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size #total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the image using `cv2.imshow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Logo image\", img)\n",
    "\n",
    "# cv2.waitKey() is a keyboard binding function.\n",
    "# The argument for waitKey is a number of milliseconds to wait for keyboard input. By\n",
    "# default, it is 0, which is a special value meaning infinity. The return value is either -1\n",
    "# (meaning that no key has been pressed) or an ASCII keycode, such as 27 for Esc.\n",
    "#waitKey only captures input when an OpenCV window has focus.\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyWindow('Logo image') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now display the image using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?? <br>\n",
    "For historical reasons, OpenCV defaults to BGR format instead of usual RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV implements literally hundreds of formulas that pertain to the conversion of color models. We can convert the BGR image to RGB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(img_gray); \n",
    "plt.colorbar();#default color map is 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_gray, cmap='gray');\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"gray image\", img_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a random image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rim=np.random.randint(0, 256, (200,300))\n",
    "plt.imshow(rim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save images using `cv2.imwrite()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('rand_im.jpg', rim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV supports a number of formats such as jpg, png, bmp, tiff,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('rand_im.bmp', rim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image operations as Numpy array operations\n",
    "Let us draw a black cross over the random image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rim[75:125, :] = 0\n",
    "rim[:, 100:200] = 0\n",
    "plt.imshow(rim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Draw letter H in a red color in blue background on a 5x4 RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[] \n",
    "h= np.empty((5,4,3), dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if `cv2.imshow()` is used to display `h`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"h\", h) #image is too small\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom window\n",
    "cv2.namedWindow('custom window', cv2.WINDOW_NORMAL ) # WINDOW_NORMAL enables you to resize the window\n",
    "cv2.imshow('custom window', h)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you will have to play with certain regions of images. It can be done with Numpy slicing. Here, I am selecting a 50x50 region on the top-left of logo.png and pasting it to the bottom right corner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=cv2.imread('data/logo.png')\n",
    "im[-50:, -50:, :] = im[:50, :50, :]\n",
    "plt.imshow(im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, we are using `cv2.imread()` and `plt.imshow()`. Hence B and R channels are reversed in the displayed image. How can you reverse the R and B channels using array operations on `im` so that `plt.imshow` shows the correct colors? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try this\n",
    "b = im[:, :, 0]\n",
    "r = im[:, :, 2]\n",
    "\n",
    "im[:, :, 0] = r\n",
    "im[:, :, 2] = b\n",
    "plt.imshow(im);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't work as expected!! What went wrong?\n",
    "In Numpy, slice of an array is a view into the same data. Not copies. Unlike Matlab.\n",
    "`b` is a view into `im[:, :, 0]`. When `im[:,:,0]` is modified, `b` is also modified.\n",
    "If we want a copy, we have to use the `copy` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = im[:, :, 0].copy()\n",
    "r = im[:, :, 2]\n",
    "im[:, :, 0] = r\n",
    "im[:, :, 2] = b\n",
    "plt.imshow(im);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic with Images\n",
    "OpenCV does *saturation arithmetic* when performing arithmetic operation on images as opposed to *modular arithmetic* done by Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([250], dtype=np.uint8)\n",
    "y = np.array([10], dtype=np.uint8)\n",
    "x + y #Numpy addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.add(x, y) #OpenCV addition-which is what we normally need with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/lena.jpg')\n",
    "\n",
    "# Convert BGR image to RGB:\n",
    "img_RGB = img[:, :, ::-1]\n",
    "plt.imshow(img_RGB);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 60 to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.full(img.shape, 60, dtype=np.uint8)\n",
    "img_add = cv2.add(img, M)\n",
    "plt.imshow(img_add[:, :, ::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Subtract 100 from all channels in all pixels in `img` using `cv2.subtract()` and display using `plt.imshow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Blending\n",
    "Image blending is also image addition, but different weights are given to the images.\n",
    "\n",
    "This function is commonly used to get the\n",
    "output from the Sobel operator.The Sobel operator is used for edge detection, where it creates an image emphasizing\n",
    "edges. The Sobel operator uses two 3 Ã— 3 kernels, which are convolved with the original\n",
    "image in order to calculate approximations of the derivatives, capturing both horizontal\n",
    "and vertical changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/pic1.jpg')\n",
    "img2 = cv2.imread('data/pic2.jpg')\n",
    "\n",
    "#alpha = 0.3, 0.7=1-0.3; make sure those values add to 1 if you want conserve brightness\n",
    "blended = cv2.addWeighted(img1, 0.3, img2, 0.7, 0)\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img1[:, :, ::-1])\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(img2[:, :, ::-1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(blended[:, :, ::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Filtering\n",
    "The `cv2.GaussianBlur()`  blurs an image by using a Gaussian kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baboon = plt.imread('data/baboon.jpg')\n",
    "\n",
    "#GaussianBlur(\tsrc, ksize, sigmaX, sigmaY,...\t)\n",
    "#when sigmaX=0, it is computed from kernel size\n",
    "babblur = cv2.GaussianBlur(baboon,(29,29),0)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(baboon)\n",
    "plt.subplot(122)\n",
    "plt.imshow(babblur);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cv2.filter2D()` function can be used to apply an arbitrary kernel to an\n",
    "image, convolving the image with the provided kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom kernel; simple box-car in this case\n",
    "kernel = np.ones((15,15))\n",
    "kernel /= kernel.size #normalize kernel so as not to scale image intensity\n",
    "\n",
    "babblur2 = cv2.filter2D(baboon,-1,kernel) #the argument -1 is for ddepth=-1; the output image will have the same depth as the source-uint8\n",
    "# each channel is processed independently\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(baboon)\n",
    "plt.subplot(122)\n",
    "plt.imshow(babblur2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other smoothing filters such as median blur and bilateral filter are also available. See the [tutorial](https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing camera frames\n",
    "The `cv2.VideoCapture()` object allows you to capture videos from different sources, such as cameras, video files and image sequences. When capturing frames from a camera connected to your computer, you have to give the camera index as the argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0) #calling the constructor, 0 is the camera index\n",
    "# Get some properties of VideoCapture using get() method\n",
    "frame_width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Print these values:\n",
    "print(f\"CV_CAP_PROP_FRAME_WIDTH: {frame_width}\")\n",
    "print(f\"CV_CAP_PROP_FRAME_HEIGHT : {frame_height}\")\n",
    "print(f\"CAP_PROP_FPS : {fps}\")\n",
    "\n",
    "ret, frame = capture.read()\n",
    "while ret:\n",
    "    cv2.imshow('Input frame from the camera', frame)\n",
    "    # Capture frame-by-frame from the camera\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    " \n",
    " \n",
    "# Release everything:\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection\n",
    "OpenCV provides an implementation of Haar cascade based face detection first proposed by Viola and Jones(2001). More details can be found [here](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html)\n",
    "The `cv2.CascadeClassifier()` function is used to load a classifier from a file. The cascade classifer files should be available  under `cv2/data` folder in your installation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face detection on still images\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "'C:/Users/ece/anaconda3/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "img = cv2.imread('data/woodcutters.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.08, 5)\n",
    "for (x, y, w, h) in faces:\n",
    "    img = cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "\n",
    "cv2.namedWindow('Woodcutters Detected!')\n",
    "cv2.imshow('Woodcutters Detected!', img)\n",
    "#cv2.imwrite('./woodcutters_detected.jpg', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `cv2.CascadeClassifier`, it makes little difference whether we perform face\n",
    "detection on a still image or a video feed. The latter is just a sequential version of the\n",
    "former. Face detection on a video is simply face detection applied to each frame. Naturally,\n",
    "with more advanced techniques, it would be possible to track a detected face continuously\n",
    "across multiple frames and determine that the face is the same one in each frame. \n",
    "\n",
    "However, it is good to know that a basic sequential approach also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "while (cv2.waitKey(1) == -1):\n",
    "    success, frame = camera.read()\n",
    "    if success:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5, minSize=(120, 120))\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.imshow('Face Detection', frame)\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Deep Learning Object Detection Examples: \n",
    "\n",
    "#### 1. Object detection using pre-trained Tensorflow model\n",
    "Deep Learning has changed Computer Vision forever!\n",
    "\n",
    "Adapted from this [article](https://towardsdatascience.com/object-detection-with-tensorflow-model-and-opencv-d839f3e42849)\n",
    "\n",
    "We are using the [EfficientDet-Lite2 ](https://tfhub.dev/tensorflow/efficientdet/lite2/detection/1) model trained on the COCO-2017 dataset for object detection. Many other models are available on [Tensorflow Hub](https://tfhub.dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/lite2/detection/1\")\n",
    "#detector = tf.saved_model.load('C:/Users/ece/src/webcam_tf')\n",
    "labels = pd.read_csv('data/labels.csv',sep=';',index_col='ID')\n",
    "labels = labels['OBJECT (2017 REL.)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    #Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #Resize to respect the input_shape\n",
    "    inp = cv2.resize(frame, (640, 480 )) # model accepts variable size images\n",
    "\n",
    "    #Convert img to RGB\n",
    "    rgb = cv2.cvtColor(inp, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  \n",
    "    rgb_tensor = tf.convert_to_tensor(rgb, dtype=tf.uint8)# model needs uint8 tensor\n",
    "\n",
    "    #Add dims to rgb_tensor\n",
    "    rgb_tensor = tf.expand_dims(rgb_tensor , 0)\n",
    "    \n",
    "    boxes, scores, classes, num_detections = detector(rgb_tensor) #other models may output in different formats\n",
    "    \n",
    "    # boxes: a tf.float32 tensor of shape [N, 4] containing bounding box coordinates in the following order: [ymin, xmin, ymax, xmax].\n",
    "    # scores: a tf.float32 tensor of shape [N] containing detection scores.\n",
    "    # classes: a tf.int tensor of shape [N] containing detection class index from the label file.\n",
    "    # num_detections: a tf.int tensor with only one value, the number of detections [N].\n",
    "    \n",
    "    pred_labels = classes.numpy().astype('int')[0]  # index by 0 to remove batch dimension\n",
    "    \n",
    "    pred_labels = [labels[i] for i in pred_labels]\n",
    "    pred_boxes = boxes.numpy()[0].astype('int')\n",
    "    pred_scores = scores.numpy()[0]\n",
    "   \n",
    "   #loop throughout the detections and place a box around it  \n",
    "    for score, (ymin,xmin,ymax,xmax), label in zip(pred_scores, pred_boxes, pred_labels):\n",
    "        if score < 0.5:\n",
    "            img_boxes = rgb\n",
    "            continue\n",
    "            \n",
    "        score_txt = f'{100 * round(score,0)}'\n",
    "        img_boxes = cv2.rectangle(rgb,(xmin, ymax),(xmax, ymin),(0,255,0),1)      \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img_boxes,label,(xmin, ymax-10), font, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(img_boxes,score_txt,(xmax, ymax-10), font, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    #Display the resulting frame\n",
    "    cv2.imshow('detections', img_boxes)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Object Detection using YOLO\n",
    "\n",
    "Adapted from [here](https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html)\n",
    "\n",
    "YOLO [homepage](https://pjreddie.com/darknet/yolo/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Coco Names\n",
    "classesFile = \"data/coco.names\"\n",
    "classNames = []\n",
    "with open(classesFile, 'r') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "print(classNames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Files\n",
    "modelConfiguration = \"C:/Users/ece/src/yolo_obj_det/yolov3.cfg\"\n",
    "modelWeights = \"C:/Users/ece/src/yolo_obj_det/yolov3.weights\"\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "whT = 320\n",
    "confThreshold =0.5\n",
    "nmsThreshold= 0.2\n",
    "\n",
    "while cv2.waitKey(1) != ord('q'):\n",
    "    success, img = cap.read()\n",
    "    blob = cv2.dnn.blobFromImage(img, 1 / 255, (whT, whT), [0, 0, 0], 1, crop=False)#pre-processing\n",
    "    net.setInput(blob)\n",
    "    layersNames = net.getLayerNames()\n",
    "    outputNames = [(layersNames[i-1]) for i in net.getUnconnectedOutLayers()]\n",
    "    outputs = net.forward(outputNames)\n",
    "    \n",
    "    hT, wT, cT = img.shape\n",
    "    bbox = []\n",
    "    classIds = []\n",
    "    confs = []\n",
    "    for output in outputs: #outputs is a tuple with len(outputs)=3 since outputs from 3 layers are taken; \n",
    "        #output = output from a layer=numpy array; no. of bounding boxes x 85\n",
    "        for det in output: #for each row of output=for each bounding box=85 element array of bounding box properties\n",
    "            # elements 0 to 3 are bounding box coordinates, det[4] is conf. that an obj is present, and det[5:] are the scores for the 80 classes\n",
    "            scores = det[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            if confidence > confThreshold:\n",
    "                w,h = int(det[2]*wT) , int(det[3]*hT)\n",
    "                x,y = int((det[0]*wT)-w/2) , int((det[1]*hT)-h/2)\n",
    "                bbox.append([x,y,w,h])\n",
    "                classIds.append(classId)\n",
    "                confs.append(float(confidence))\n",
    " \n",
    "    indices = cv2.dnn.NMSBoxes(bbox, confs, confThreshold, nmsThreshold)#indices = #the kept indices of bboxes after NMS.\n",
    " \n",
    "    for i in indices:\n",
    "        #i = i[0]\n",
    "        box = bbox[i]\n",
    "        x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "        # print(x,y,w,h)\n",
    "        cv2.rectangle(img, (x, y), (x+w,y+h), (255, 0 , 255), 2)\n",
    "        cv2.putText(img,f'{classNames[classIds[i]].upper()} {int(confs[i]*100)}%',\n",
    "                  (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    " \n",
    "    cv2.imshow('Image', img)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV also allows loading DNN models from many frameworks such as Tensorflow, Pytorch etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cv2.dnn.readNetFromTensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving camera frames:\n",
    "`cv2.VideoWriter` object can be used to write frames to a video file. The video's file name and codec must be specified as arguments to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "# Get some properties of VideoCapture (frame width, frame height and frames per second (fps)):\n",
    "frame_width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "writer = cv2.VideoWriter('MyOutputVid.mp4', \n",
    "cv2.VideoWriter_fourcc(*'MP4V'), # FourCC is a 4-byte code used to specify the video codec-file ext and codec should match\n",
    "int(fps), (int(frame_width), int(frame_height)))\n",
    "\n",
    "# videoWriter = cv2.VideoWriter(\n",
    "# 'MyOutputVid1.avi', cv2.VideoWriter_fourcc('I','4','2','0'),\n",
    "# int(fps), (int(frame_width), int(frame_height)))\n",
    "\n",
    "frame_number=0\n",
    "while frame_number < 150:\n",
    "    # Capture frame-by-frame from the camera\n",
    "    ret, frame = capture.read()    \n",
    "    writer.write(frame)\n",
    "\n",
    "    frame_number +=1\n",
    " \n",
    "# Release everything:\n",
    "capture.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a video file\n",
    "`cv2.VideoCapture` also allows us to read a video file. To read a video file, the\n",
    "path to the video file should be passed instead of the camera's device index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('MyOutputVid.mp4')\n",
    "\n",
    "ret, frame = capture.read()\n",
    "\n",
    "while ret:\n",
    "    cv2.imshow('Frame from video file', frame)\n",
    "    ret, frame = capture.read()    \n",
    "       \n",
    "    if cv2.waitKey(33) == ord('q'): #30 frames per 1000 ms ~= 33 ms per frame\n",
    "        break\n",
    " \n",
    "# Release everything:\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny Edge Detection\n",
    "(https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('data/lena.jpg')\n",
    "canny_edge= cv2.Canny(img, 100, 200)\n",
    "plt.imshow(canny_edge, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Scripts\n",
    "Many sample programs are included in the OpenCV's source code archive. To dowload the source code, go to (https://opencv.org/releases/) and download **Sources**. It is a zip file (90 MB). Unzip it and find the samples scripts in `opencv/samples/python` folder.\n",
    "Try running a sample program, for example, `hist.py`.\n",
    "\n",
    "Note that many of the sample scripts require command line arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many interesting OpenCV projects are on [this](https://www.youtube.com/channel/UCYUjYU5FveRAscQ8V21w81A) Youtube Channel"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62882779011e7f5c6797d1c669bfb05c908ed0af04b3e8ec08bc96756e80d482"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
